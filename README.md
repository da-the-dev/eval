# Evaluation framework
The goal of this project is to create an evaluation toolkit to measure LLM performance.

Key notable metrics:
- Language metrics
  - Perplexity
  - BLUE
  - ROUGE
  - Other metrics (TODO Define)
- Task-specific metrics
  - Different benchmarks (TODO Define what benchmarks)
- Inference metrics
  - Time To First Token (TTFT)
  - Memory footprint

This framework is to be used in a future work implementing prunning-training approach to model compression
