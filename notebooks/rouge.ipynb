{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8b5fc7",
   "metadata": {},
   "source": [
    "# ROUGE\n",
    "Recall-Oriented Understudy for Gisting Evaluation\n",
    "\n",
    "Before we start, there are about fifty gazzilion variations of ROUGE\n",
    "\n",
    "However, all ROUGE-s rely on recall, precision, and F1 score\n",
    "\n",
    "## Theory: Basics\n",
    "> $\n",
    "> \\text{Precision} = \\dfrac{\\text{Relevant retrieved instances}}{\\text{All }\\textbf{retrieved}\\text{ instances}}\n",
    "> $\n",
    "> \n",
    "> \"I found 5 pages with text relevant to the query out of 7 total pages\"\n",
    "> \n",
    "> \"I hit 5 targets on the dot out of 7 total targets\"\n",
    "\n",
    "> $\n",
    "> \\text{Recall} = \\dfrac{\\text{Relevant retrieved instances}}{\\text{All }\\textbf{relevant}\\text{ instances}}\n",
    "> $\n",
    "> \"I recalled 5 memories out of 7 total **relevant** memories\"\n",
    "\n",
    "Both recall and precision divide **relevant retrieved** instances. The nominator is always the True Positive. The only difference is in the denominator.\n",
    "\n",
    "> $\n",
    "> F1 = \\dfrac{2(Precision \\times Recall)}{Precision + Recall}\n",
    "> $\n",
    ">\n",
    "> This is a balanced way to combine Precision and Recall\n",
    "\n",
    "## ROUGE: Basics\n",
    "In essence, ROUGE is a glorified F1 score that specifically computes matches between reference and candidate texts.\n",
    "\n",
    "\n",
    "### ROUGE-1\n",
    "We can take 2 candidate texts and 1 reference text\n",
    "- Reference: \"The cat sat on the mat.\"\n",
    "- Candidate 1: \"The cat sat.\"\n",
    "- Candidate 2: \"The cat sat on the mat, and the dog barked at the moon, and the cow jumped over the sun.\"\n",
    "\n",
    "Candidate 1:\n",
    "\n",
    "> **Precision:**  3 / 3 = 1.0. All 3 of the words used were in the reference.\n",
    "> \n",
    "> **Recall:** 3 / 6 = 0.5. 3 out of the 6 words were captured from the reference.\n",
    "\n",
    "Now, for candidate 2:\n",
    "\n",
    "> **Precision:** 6 / 21 â‰ˆ 0.29. Only 6 out of 21 total words were relevant.\n",
    ">\n",
    "> **Recall:** 6 / 6 = 1.0. 6 out of the 6 words were captured from the reference.\n",
    "\n",
    "We can obviously see that F1 score will be much lower for candidate 2 compared to candidate 1. Candidate 1 is better. This makes sense, since the first candidate capture most of the relevant meaning without adding extra.\n",
    "\n",
    "### ROUGE-2 & ROUGE-N\n",
    "ROUGE-1 has an issue with this case:\n",
    "\n",
    "> **Reference:** \"The president signed the new bill.\"\n",
    "> \n",
    "> **Candidate:** \"The bill signed the new president.\"\n",
    "\n",
    "ROUGE-1 would say that the candidate reference is ideal complement for the reference. But the meaning is completely lost.\n",
    "\n",
    "An improvement, ROUGE-2 (and ROUGE-N by extension) uses the same idea, but splits the sentence into bigrams (or n-grams respectively). Now:\n",
    "\n",
    "> **Reference Bigrams:** \"the president\", \"president signed\", \"signed the\", \"the new\", \"new bill\"\n",
    ">\n",
    "> **Candidate Bigrams:** \"the bill\", \"bill signed\", \"signed the\", \"the new\", \"new president\"\n",
    "\n",
    "Now, F1 score would obviously show that the candidate summary is actually bad for the given reference.\n",
    "\n",
    "A generalization is ROUGE-N. The larger the N, the more context is captured. Note that bigger n-grams make more grams and this requires more comparisons. So, this is a trade-off\n",
    "\n",
    "### ROUGE-L\n",
    "ROUGE-N is good, but if the summary is short and good (i.e. keeps the main idea, but throws out details), ROUGE-N will penalize this behavior. Example:\n",
    "\n",
    "> **Reference:** \"The extremely quick brown fox jumps over the very lazy dog.\"\n",
    "> \n",
    "> **Candidate:** \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "The only difference is \"extremely\", and ROUGE-N will penalize this. But the candidate is still quite good. A metric that would favour this kind of cases is Longest Common Subsequence. This allows \"gaps\" in the candidate, as long as the candidate has the same words in the same order.\n",
    "\n",
    "Using this metric requires modifying Precision and Recall.\n",
    "\n",
    "> $\n",
    "> \\text{Recall-LCS} = \\dfrac{\\text{Length of LCS}}{\\text{Total words in the reference summary}}\n",
    "> $\n",
    "\n",
    "Tells us what fraction of the reference summary is covered by the common subsequence.\n",
    "\n",
    "> $\n",
    "> \\text{Precision-LCS} = \\dfrac{\\text{Length of LCS}}{\\text{Total words in the candidate summary}}\n",
    "> $\n",
    "\n",
    "Tells us how much of the candidate summary is relevant and in the correct order.\n",
    "\n",
    "Formula for F1 score is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b18a530",
   "metadata": {},
   "source": [
    "With theory out the way, let's implement all of these ROUGE-s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf58668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def preprocess(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    This is your first and most important helper function.\n",
    "    Before you can compare words, you need a clean, consistent list of them.\n",
    "    What steps should you take to turn a raw string into a list of \"tokens\"?\n",
    "    \n",
    "    Hint: Think about case sensitivity and how to split the sentence into words.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^A-z\\s\\d][\\\\\\^]?', '', text).lower().split()\n",
    "\n",
    "def _f1_score(precision: float, recall: float) -> float:\n",
    "    \"\"\"\n",
    "    A helper function to calculate the F1-score.\n",
    "    You already know the formula for this.\n",
    "    What is one edge case you should be careful of to avoid a division error?\n",
    "    \"\"\"\n",
    "    if precision + recall == 0:\n",
    "        raise ValueError(f\"Precision + recall cannot be 0, division by zero. Precision: {precision}, recall: {recall}\")\n",
    "    \n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def _create_ngrams(tokens: list[str], n: int) -> set[tuple]:\n",
    "    \"\"\"\n",
    "    This function will be used by ROUGE-N.\n",
    "    It needs to take a list of tokens and slide a window of size 'n' across it.\n",
    "    \n",
    "    For example, with tokens ['a', 'b', 'c', 'd'] and n=2,\n",
    "    it should produce something like {('a', 'b'), ('b', 'c'), ('c', 'd')}.\n",
    "    \n",
    "    Why might returning a 'set' of n-grams be more efficient than a 'list'?\n",
    "    \"\"\"\n",
    "    ngrams = set()\n",
    "    for i in range(0, len(tokens) + 1 - n):\n",
    "        start_ngram = i\n",
    "        end_ngram = i + n\n",
    "        \n",
    "        ngrams.add(tuple(tokens[start_ngram : end_ngram]))\n",
    "        \n",
    "    return ngrams\n",
    "        \n",
    "\n",
    "def calculate_rouge_n(candidate_tokens: list[str], reference_tokens: list[str], n: int) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    This function will calculate ROUGE-N (e.g., ROUGE-1, ROUGE-2).\n",
    "    \n",
    "    Here's the plan:\n",
    "    1. Use your `_create_ngrams` helper to get the n-grams for both the candidate and reference.\n",
    "    2. Find the number of n-grams that are present in BOTH sets. This is your overlap.\n",
    "    3. Calculate precision: How many of the candidate's n-grams were relevant?\n",
    "       (Hint: overlap / total number of candidate n-grams)\n",
    "    4. Calculate recall: How many of the reference's n-grams were captured?\n",
    "       (Hint: overlap / total number of reference n-grams)\n",
    "    5. Use your `_f1_score` helper to combine precision and recall.\n",
    "    6. Return all three scores in a dictionary: {\"precision\": p, \"recall\": r, \"f1\": f1}.\n",
    "    \"\"\"\n",
    "    candidate_ngram = _create_ngrams(candidate_tokens, n)\n",
    "    reference_ngram = _create_ngrams(reference_tokens, n)\n",
    "    \n",
    "    overlap = candidate_ngram.intersection(reference_ngram)\n",
    "    \n",
    "    p = len(overlap) / len(candidate_ngram) if len(candidate_ngram) > 0 else 0.0\n",
    "    r = len(overlap) / len(reference_ngram) if len(reference_ngram) > 0 else 0.0\n",
    "\n",
    "    f1 = _f1_score(p, r)\n",
    "    \n",
    "    return {\"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "    \n",
    "    \n",
    "\n",
    "def _lcs_length(X: list[str], Y: list[str]) -> int:\n",
    "    \"\"\"\n",
    "    This is the heart of ROUGE-L. It calculates the Length of the Longest Common Subsequence.\n",
    "    This is a classic dynamic programming problem.\n",
    "    \n",
    "    Imagine creating a grid (or a 2D array) where the rows correspond to tokens in X\n",
    "    and columns correspond to tokens in Y.\n",
    "    \n",
    "    The value in each cell `grid[i][j]` will represent the length of the LCS\n",
    "    for the subsequences `X[:i]` and `Y[:j]`.\n",
    "    \n",
    "    How would you fill in the value of `grid[i][j]` based on whether `X[i-1]` and `Y[j-1]` match?\n",
    "    - If `X[i-1] == Y[j-1]`, it means we have found a common element. The length of the LCS\n",
    "      is one more than the LCS of the subsequences before this match.\n",
    "      So, `grid[i][j] = grid[i-1][j-1] + 1`.\n",
    "\n",
    "    What do you do if they don't match?\n",
    "    - If `X[i-1] != Y[j-1]`, the LCS is the longest of the two possibilities:\n",
    "      1. The LCS of `X[:i]` and `Y[:j-1]` (i.e., `grid[i][j-1]`).\n",
    "      2. The LCS of `X[:i-1]` and `Y[:j]` (i.e., `grid[i-1][j]`).\n",
    "      So, `grid[i][j] = max(grid[i-1][j], grid[i][j-1])`.\n",
    "    \n",
    "    The final answer will be the number in the bottom-right corner of the grid.\n",
    "    \"\"\"\n",
    "    m, n = len(X), len(Y)\n",
    "    \n",
    "    # Initialize the DP grid with zeros.\n",
    "    # grid[i][j] will store the length of LCS of X[:i] and Y[:j]\n",
    "    grid = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    # Build the grid in a bottom-up fashion\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if X[i - 1] == Y[j - 1]:\n",
    "                # If the current tokens match, extend the LCS from the previous diagonal cell.\n",
    "                grid[i][j] = grid[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                # If they don't match, take the maximum LCS length from the top or left cell.\n",
    "                grid[i][j] = max(grid[i - 1][j], grid[i][j - 1])\n",
    "                \n",
    "    # The value in the bottom-right corner is the length of the LCS for the entire sequences.\n",
    "    return grid[m][n]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_rouge_l(candidate_tokens: list[str], reference_tokens: list[str]) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    This function will calculate ROUGE-L.\n",
    "    \n",
    "    The plan is very similar to ROUGE-N, but uses LCS instead of n-gram overlap:\n",
    "    1. Use your `_lcs_lenlength` helper to find the length of the longest common subsequence.\n",
    "    2. The denominators for precision and recall are simpler here. What are they?\n",
    "       (Hint: think about the total number of words in each summary).\n",
    "    3. Calculate Precision-LCS and Recall-LCS.\n",
    "    4. Use `_f1_score` to get the final F1 score.\n",
    "    5. Return the scores in a dictionary.\n",
    "    \"\"\"\n",
    "    lcs = _lcs_length(candidate_tokens, reference_tokens)\n",
    "    \n",
    "    p = lcs / len(candidate_tokens)\n",
    "    r = lcs / len(reference_tokens)\n",
    "    \n",
    "    f1 = _f1_score(p, r)\n",
    "    \n",
    "    return {\"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a95745c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Scores: {'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n",
      "ROUGE-2 Scores: {'precision': 0.4, 'recall': 0.4, 'f1': 0.4000000000000001}\n",
      "ROUGE-L Scores: {'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f1': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "# Once you've implemented the functions, you can test them like this.\n",
    "\n",
    "reference_text = \"The president signed the new bill.\"\n",
    "candidate_text = \"The bill signed the new president.\"\n",
    "\n",
    "# 1. Preprocess the texts\n",
    "reference_tokens = preprocess(reference_text)\n",
    "candidate_tokens = preprocess(candidate_text)\n",
    "\n",
    "# 2. Calculate ROUGE-1\n",
    "rouge1_scores = calculate_rouge_n(candidate_tokens, reference_tokens, 1)\n",
    "print(f\"ROUGE-1 Scores: {rouge1_scores}\")\n",
    "\n",
    "# 3. Calculate ROUGE-2\n",
    "rouge2_scores = calculate_rouge_n(candidate_tokens, reference_tokens, 2)\n",
    "print(f\"ROUGE-2 Scores: {rouge2_scores}\")\n",
    "\n",
    "# 4. Calculate ROUGE-L\n",
    "rougel_scores = calculate_rouge_l(candidate_tokens, reference_tokens)\n",
    "print(f\"ROUGE-L Scores: {rougel_scores}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
