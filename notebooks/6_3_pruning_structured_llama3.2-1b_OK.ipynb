{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6-PRUNING/6_3_pruning_structured_llama3.2-1b_OK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT0FAsUnncFY"
      },
      "source": [
        "<div>\n",
        "    <h1>Large Language Models Projects</a></h1>\n",
        "    <h3>Apply and Implement Strategies for Large Language Models</h3>\n",
        "    <h2>Pruning Llama 3.2.</h2>\n",
        "    <h3>Example of approach to pruning a Llama Model.</h3>\n",
        "</div>\n",
        "\n",
        "by [Pere Martra](https://www.linkedin.com/in/pere-martra/)\n",
        "\n",
        "_______\n",
        "Contributions:\n",
        "- [Mariusz Kurman](https://www.linkedin.com/in/mariuszkurman/). Improved the `compute_neuron_pair_importance` function, adding the absolute min value to the equation to evaluate the neurons.\n",
        "_______\n",
        "Models: meta-llama/Llama-3.2-1B\n",
        "\n",
        "Colab Environment: GPU T4.\n",
        "\n",
        "Keys:\n",
        "* Pruning\n",
        "* Structured pruning\n",
        "\n",
        "\n",
        "Related article: [How to Prune LLaMA 3.2 and Similar Large Language Models](ttps://medium.com/towards-data-science/how-to-prune-llama-3-2-and-similar-large-language-models-cf18e9a2afb6.)\n",
        "_______\n",
        "**disclaimer: The pruning section was created after the first edition of the book was published. They are not included in the bookâ€™s original content but are intended to supplement and expand on the topics covered.**\n",
        "\n",
        "This is the unofficial repository for the book:\n",
        "        <a href=\"https://amzn.to/4eanT1g\"> <b>Large Language Models:</b> Apply and Implement Strategies for Large Language Models</a> (Apress).\n",
        "        The book is based on the content of this repository, but the notebooks are being updated, and I am incorporating new examples and chapters.\n",
        "        If you are looking for the official repository for the book, with the original notebooks, you should visit the\n",
        "        <a href=\"https://github.com/Apress/Large-Language-Models-Projects\">Apress repository</a>, where you can find all the notebooks in their original format as they appear in the book.\n",
        "\n",
        "This notebook serves as a demonstration code for the paper [Exploring GLU Expansion Ratios: Structured Pruning in Llama-3.2 Models.](https://doi.org/10.31219/osf.io/qgxea)\n",
        "\n",
        "The paper studies how the % of expansion produced in the GLU layers influences performance and consumption. For this purpose, seven different models have been generated from the Llama-3.2-1B and Llama-3.2-3B base models, reaching the conclusion that the optimal balance is achieved with an expansion of 140%.\n",
        "______"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEwxZCVsoIau"
      },
      "source": [
        "# Introduction\n",
        "This notebook cotinues the work done at: [6_2_pruning_structured_llama3.2-1b_KO.ipynb](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6_2_pruning_structured_llama3.2-1b_KO.ipynb) where an incorrect pruning was applied to a Llama3.2 model.\n",
        "\n",
        "The pruning process was based on selecting neurons from the model's MLP layers that have the least importance using the L1 norm, assuming these contributed the least to the model's output.\n",
        "\n",
        "However, by ignoring the model's structure, some problems arose, which are addressed in this notebook, by taking the actions:\n",
        "\n",
        "* Consider the GLU (Gated Linear Unit) structure of the MLP layers.\n",
        "* Use a neuron selection method that is compatible with the GLU structure.\n",
        "\n",
        "In this notebook, we focus on explaining the modifications made to the pruning process that have successfully allowed us to create a smaller model while retaining almost all the functionalities of the base model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQIxAOPZtPBN"
      },
      "source": [
        "#Install libraries & Configure variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zHApVm41HWq",
        "outputId": "015adf1f-b283-41f0-c204-325ed906bd47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q torch\n",
        "!pip install -q datasets\n",
        "!pip install -q sentencepiece  # Required for LLaMA tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GJNgRj4M187E"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbIyUlXEtbqs",
        "outputId": "661e4946-0053-47a6-b91e-e55d5d12eaa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "device = torch.device('mps')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM-QwxyKw-YG"
      },
      "source": [
        "#Download model and explore structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "q-z_1Zpg2I6u"
      },
      "outputs": [],
      "source": [
        "model_name = 'HuggingFaceTB/SmolLM2-135M'\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#tokenizer.pad_token = tokenizer.eos_token  # Set pad token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "9UpMD4Hw2MWg"
      },
      "outputs": [],
      "source": [
        "def get_output(prompt, model=model, tokenizer=tokenizer):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
        "    outputs = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        attention_mask=inputs['attention_mask'],\n",
        "        max_length=50,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        temperature=None,\n",
        "        top_p=None,\n",
        "        do_sample=False,          # Disable sampling\n",
        "        num_beams=5,              # Use beam search\n",
        "        early_stopping=True,      # Stop when end-of-sequence token is generated\n",
        "        no_repeat_ngram_size=2    # Prevent repetition of 2-grams\n",
        "    )\n",
        "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4muyx_8M5OAu"
      },
      "source": [
        "## studying the model structure\n",
        "As demonstrated in the [previous notebook](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6_2_pruning_structured_llama3.2-1b_KO.ipynb), studying the structure of the model that will undergo pruning is crucial.\n",
        "\n",
        "In this notebook, weâ€™re going to fine-tune the pruning process for the Llama3.2 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5Hs4oQ4B7Z0",
        "outputId": "61ccc5f9-26a9-4b52-ee7b-9a5c487a00dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(49152, 576)\n",
            "    (layers): ModuleList(\n",
            "      (0-29): 30 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
            "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPMslK3QCAb1"
      },
      "source": [
        "\n",
        "An MLP block typically consists of layers that scale the data to larger dimensions and others that return it to its original size.\n",
        "\n",
        "In the MLP block of the model, we find two projection layers: `gat_proj` and `down_proj`, both scaling from 2048 to 8192. The purpose of having two layers projecting to the same intermediate size might be related to gating mechanisms. A gating mechanism selectively controls information flow in neural networks by using learned weights to \"gate\" or filter inputs.\n",
        "\n",
        "However, to truly understand how these layers function, weâ€™d need to refer to the model's documentation or even the source code. But, this structure usually indicates, at least, I haven't encountered a case where it doesn't, that the layers performing the upsizing work in pairs, and they cannot be treated as independent linear layers.\n",
        "\n",
        "In other words, any operation we apply to one layer must be replicated in the other. Most importantly, when identifying which neurons have more or less importance, we can't evaluate the neurons of a single layer in isolation; we need to treat them as pairs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alKH3QH64WFL",
        "outputId": "839eae37-b7e4-436d-ad91-c552701200de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text: Paris is the capital of France and the largest city in the country. It is located on the banks of the Seine River. The city has a population of about 1.5 million people.\n",
            "\n",
            "The history of Paris dates back to the\n"
          ]
        }
      ],
      "source": [
        "# Test the original model\n",
        "prompt = \"Paris is the capital of\"\n",
        "generated = get_output(prompt)\n",
        "print(f\"Generated text: {generated}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "8WR96iwq2XYH"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kph43oObnet7",
        "outputId": "c77780b4-b42e-4033-96e0-68e345d288cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original model parameters: 134515008\n"
          ]
        }
      ],
      "source": [
        "original_param_count = count_parameters(model)\n",
        "print(f\"Original model parameters: {original_param_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK9NwmBWnkSP"
      },
      "source": [
        "#Pruning the Model.\n",
        "##Support pruning functions.\n",
        "###Compute neuron importance functions.\n",
        "\n",
        "Here are three functions I used to calculate neuron importance, allowing us to decide which ones to eliminate.\n",
        "\n",
        "All three functions take into account that the layers should be treated as pairs, considering both layers to calculate neuron importance.\n",
        "\n",
        "The results obtained with each function have been quite different:\n",
        "\n",
        "* **Product of Norms**: Paris is the capital of of of of the of the the the the to to to from to from from from to to from to\n",
        "France France France France France France France France France France France\n",
        "France France France France France\n",
        "\n",
        "* **Variance of weights**: Paris is the capital of the French Republic. It is also a...\n",
        "Paris is the capital of the French Republic. It is also a\n",
        "Germany is the German Republic. It is also a\n",
        "of the Austrian Republic. It is also a\n",
        "\n",
        "* **Maximum absolute weight**: Paris is the capital of France. It is also one of the most beautiful cities in the world. There is so much to see and do in Paris that it is impossible to cover it all in one day. However, there are a few things you should not miss while you\n",
        "\n",
        "* **Base model**: Paris is the capital of France and one of the most visited cities in the world. It is a city with a rich history and culture, as well as a vibrant and diverse population. Paris is home to many famous landmarks, including the Eiff\n",
        "\n",
        "It seems clear that the **Absolute Maximum** calculation has worked the best. I'd say the other methods for selecting neurons to remove have severely degraded the model, or at least eliminated a significant portion of the base model's capabilities.\n",
        "\n",
        "*Iâ€™m leaving the others in the notebook purely as an exercise.*\n",
        "\n",
        "The **Maximum Absolute Weight** method works better because it directly identifies the most influential neurons based on the magnitude of their connections. These neurons are likely responsible for key decisions, making the model more accurate after pruning. The Variance of Weights method, while useful in some contexts, can retain neurons that may not contribute significantly to the task, leading to less coherent model outputs.\n",
        "\n",
        "However, we shouldnâ€™t fall into the trap of assuming that this neuron selection method will work best across all model structures. It works well with Llama models, and this may be due to several factors:\n",
        "\n",
        "* The relatively large projection from 2048 to 8192.\n",
        "* The use of a GLU structure.\n",
        "* The type of activation function used.\n",
        "\n",
        "So, if we use a model from another family, like Gemma or Mistral, the neuron selection method might need to be entirely different.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "fflgE2Y8_eZF"
      },
      "outputs": [],
      "source": [
        "#****DISCARTED****\n",
        "#Product of Norms:\n",
        "#Since the GLU multiplies the outputs of gate_proj and up_proj,\n",
        "#we can compute the product of their weight norms to better represent the\n",
        "#importance of the neuron pair\n",
        "def compute_neuron_pair_importance(gate_weight, up_weight):\n",
        "\n",
        "    gate_norms = torch.norm(gate_weight, p=1, dim=1)\n",
        "    up_norms = torch.norm(up_weight, p=1, dim=1)\n",
        "    importance_scores = gate_norms * up_norms\n",
        "    return importance_scores\n",
        "#sample response: Paris is the capital of of of of the of the the the the to to to from to from from from to to from to\n",
        "#France France France France France France France France France France France\n",
        "#France France France France France\n",
        "#All All\n",
        "#All"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "_mMGhTtD4PRX"
      },
      "outputs": [],
      "source": [
        "# #****DISCARTED****\n",
        "# #Variance of Weights\n",
        "# #Neurons with higher weight variance may contribute more to the model's output.\n",
        "# def compute_neuron_pair_importance(gate_weight, up_weight):\n",
        "#     gate_variance = torch.var(gate_weight, dim=1)\n",
        "#     up_variance = torch.var(up_weight, dim=1)\n",
        "#     importance_scores = gate_variance + up_variance\n",
        "#     return importance_scores\n",
        "# #sample response: Paris is the capital of the French Republic. It is also a...\n",
        "# #Paris is the capital of the French Republic. It is also a\n",
        "# #Germany is the German Republic. It is also a\n",
        "# #of the Austrian Republic. It is also a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Seyqaquj7mQA"
      },
      "outputs": [],
      "source": [
        "# #****SELECTED****\n",
        "# #Maximum Absolute Weight:\n",
        "# #The maximum absolute weight in a neuron might indicate its significance.\n",
        "\n",
        "# def compute_neuron_pair_importance(gate_weight, up_weight):\n",
        "#   \"\"\"\n",
        "#   compute neuron pair importance scores (Maximum Absolute Weight)\n",
        "\n",
        "#   Args:\n",
        "#   - gate_weight: Weight matrix from the gate_proj layer.\n",
        "#   - up_weight: Weight matrix from the up_weight layer.\n",
        "\n",
        "#   Returns:\n",
        "#   - importance_scores: Importance scores for each neuron pair.\n",
        "#   \"\"\"\n",
        "\n",
        "#   gate_max_abs = torch.max(gate_weight, dim=1).values + torch.abs(torch.min(gate_weight, dim=1).values)\n",
        "#   up_max_abs = torch.max(up_weight, dim=1).values + torch.abs(torch.min(up_weight, dim=1).values)\n",
        "#   importance_scores = gate_max_abs + up_max_abs\n",
        "#   return importance_scores\n",
        "\n",
        "# #response: Paris is the capital of France. It is also one of the most beautiful cities in the world. There is so much to see and do in Paris that it is impossible to cover it all in one day. However, there are a few things you should not miss while you\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "NX9Boph94RWA"
      },
      "outputs": [],
      "source": [
        "#Prunes a specific percentatge of neurons from the MLP (feed forward layers).\n",
        "def prune_neuron_pairs(mlp, prune_percent):\n",
        "    \"\"\"\n",
        "    Reduces the dimensions of the **gate_proj**,**up_proj**, **down_proj**\n",
        "    layers removing the least important neurons.\n",
        "\n",
        "    Args:\n",
        "    - mlp: Layers to prune.\n",
        "    - prune_percent: Percentage of neurons to prune.\n",
        "\n",
        "    Returns:\n",
        "    - new_gate_proj, new_up_proj, new_down_proj:  New pruned layers.\n",
        "    - k: New intermediate size.\n",
        "\n",
        "    \"\"\"\n",
        "    # Extract the weights from the MLP layers\n",
        "    #  these weights are used to calculate each neuron's\n",
        "    #  importance score in the next step.\n",
        "    gate_weight = mlp.gate_proj.weight.data.float()\n",
        "    up_weight = mlp.up_proj.weight.data.float()\n",
        "\n",
        "    #Compute importance stores. Neurons with higher importance scores\n",
        "    # are considered more important and less likely to be pruned.\n",
        "    importance_scores = compute_neuron_pair_importance(gate_weight, up_weight)\n",
        "\n",
        "    #Store the original number of neurons in the intermediate layer.\n",
        "    original_intermediate_size = gate_weight.size(0)\n",
        "    #Computes the number of neurons to prune.\n",
        "    num_neuron_pairs_to_prune = min(int(prune_percent * original_intermediate_size), original_intermediate_size - 1)\n",
        "    #Calculate the number of neurons to keep. The new intermediate size.\n",
        "    k = original_intermediate_size - num_neuron_pairs_to_prune\n",
        "\n",
        "    #Just check that there is no big error calculating k. We can't prune all the neurons.\n",
        "    if k <= 0:\n",
        "        raise ValueError(f\"Invalid number of neuron pairs to keep: {k}. Adjust the prune_percent.\")\n",
        "\n",
        "    #Select the neuros to keep, by obtaining the indices to keep.\n",
        "    _, indices_to_keep = torch.topk(importance_scores, k, largest=True, sorted=True)\n",
        "    indices_to_keep = indices_to_keep.sort().values\n",
        "\n",
        "    #create the new layers\n",
        "    new_gate_proj = nn.Linear(mlp.gate_proj.in_features, k, bias=False).to(device)\n",
        "    new_up_proj = nn.Linear(mlp.up_proj.in_features, k, bias=False).to(device)\n",
        "    new_down_proj = nn.Linear(k, mlp.down_proj.out_features, bias=False).to(device)\n",
        "\n",
        "    #copy weights to the new layers.\n",
        "    new_gate_proj.weight.data = mlp.gate_proj.weight.data[indices_to_keep, :]\n",
        "    new_up_proj.weight.data = mlp.up_proj.weight.data[indices_to_keep, :]\n",
        "    new_down_proj.weight.data = mlp.down_proj.weight.data[:, indices_to_keep]\n",
        "\n",
        "    #return new layers and intermediate size.\n",
        "    return new_gate_proj, new_up_proj, new_down_proj, k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT0v_RpeST87"
      },
      "source": [
        "# Prune Loop\n",
        "The update_model function iterates through the blocks within the model's Transformer structure. This structure consists of multiple `LlamaDecoderLayer` blocks, and each of these blocks contains a pair of `LlamaSdpaAttention` and `LlamaMLP` components. The latter contains the MLP layers that will be the target of the pruning process.\n",
        "```\n",
        "(layers): ModuleList(\n",
        "      (0-15): 16 x LlamaDecoderLayer(\n",
        "        (self_attn): LlamaSdpaAttention(\n",
        "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
        "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
        "          (rotary_emb): LlamaRotaryEmbedding()\n",
        "        )\n",
        "        (mlp): LlamaMLP(\n",
        "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
        "          (act_fn): SiLU()\n",
        "        )\n",
        "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
        "      )\n",
        "  )    \n",
        "```\n",
        "The layers that will undergo the removal of neurons identified as less useful are:\n",
        "```\n",
        "(gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "(up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
        "(down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
        "```\n",
        "The neurons are removed in the `prune_neurons` function based on the values returned by `compute_neuron_pair_importance`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "FxJEWg1X3j0m"
      },
      "outputs": [],
      "source": [
        "#Iterates throught the model layers and applies pruning.\n",
        "def update_model(model, prune_percent):\n",
        "    \"\"\"\n",
        "    It modifies each mlp layer present in model, to retain only the most\n",
        "    important neurons. Creating new smaller versions of each layer pruned.\n",
        "\n",
        "    Args:\n",
        "    - model: Model to prune.\n",
        "    - prune_percent: Percentage of neurons to prune.\n",
        "\n",
        "    Returns:\n",
        "    - model: New pruned model.\n",
        "    \"\"\"\n",
        "    new_intermediate_size = None\n",
        "\n",
        "    #loop for each model layer.\n",
        "    for idx, layer in enumerate(model.model.layers):\n",
        "        #Since each layer is a LlamaDecoderLayer it contains multiple components\n",
        "        # Attention, MLP and Layer norms. We're targetting MLP component\n",
        "        # by accesing layer.mlp.\n",
        "        mlp = layer.mlp\n",
        "\n",
        "        #Call the prune_neiron_pairs with the layers and receiving the pruned.\n",
        "        new_gate_proj, new_up_proj, new_down_proj, new_size = prune_neuron_pairs(mlp, prune_percent)\n",
        "\n",
        "        #Replace the Origiginal Layers with Pruned Layers.\n",
        "        mlp.gate_proj = new_gate_proj\n",
        "        mlp.up_proj = new_up_proj\n",
        "        mlp.down_proj = new_down_proj\n",
        "\n",
        "        #new_intermediate_size only needs to be set once\n",
        "        if new_intermediate_size is None:\n",
        "            new_intermediate_size = new_size\n",
        "\n",
        "    #Update the model config file.\n",
        "    model.config.intermediate_size = new_intermediate_size\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtHtSbRmS267"
      },
      "source": [
        "## Obtain & test the pruned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "NIUnFU5R3n42"
      },
      "outputs": [],
      "source": [
        "prune_percent = 0.2  # Prune 20% of neurons\n",
        "model = update_model(model, prune_percent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdJUkfWI3qMM",
        "outputId": "b23e2414-25f4-4c96-eb99-afecd48eea20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pruned model parameters: 118600128\n",
            "Reduction in parameters: 15914880\n",
            "Percentage of weight savings: 11.83%\n"
          ]
        }
      ],
      "source": [
        "# Recalculate the number of parameters\n",
        "pruned_param_count = count_parameters(model)\n",
        "reduction_in_params = original_param_count - pruned_param_count\n",
        "percentage_savings = (reduction_in_params / original_param_count) * 100\n",
        "\n",
        "print(f\"Pruned model parameters: {pruned_param_count}\")\n",
        "print(f\"Reduction in parameters: {reduction_in_params}\")\n",
        "print(f\"Percentage of weight savings: {percentage_savings:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvj-iIsO5M6U",
        "outputId": "e0d470bf-c7cd-4fbf-ec9f-4381e154e9f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated text after pruning: Paris is the capital of...â€¦  \n",
            "\n",
            "Â \n",
            " ...\n",
            ",Â , \"Â  \", ... necessit,, or--\"--\n",
            "--The--\n",
            "   -- necessitÂ --ï¿½...numerable---------)]--Â Â \n"
          ]
        }
      ],
      "source": [
        "# Test the pruned model\n",
        "generated = get_output(prompt, model, tokenizer)\n",
        "print(f\"Generated text after pruning: {generated}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGzXMQrVTULv"
      },
      "source": [
        "The result is slightly different from what the original model produced, but itâ€™s still a fairly accurate response.\n",
        "\n",
        "In contrast to the model created in notebook: [6_2_pruning_structured_llama3.2-1b_KO.ipynb](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/6_2_pruning_structured_llama3.2-1b_KO.ipynb) where the pruned Llama model lost almost all its utility, the model in this notebook retains a good portion of its knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDQrSrf-VCyI"
      },
      "source": [
        "Looking at the modelâ€™s new structure, we can see that the `gate_proj` and `up_proj` layers have had their `out_features` reduced to 6554 from 8192. Consequently, the `down_proj` layer has its `in_features` adjusted to match the new size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATAiqZW30NYN",
        "outputId": "a124260f-8192-477b-9e16-f0e7bbe96df4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlamaForCausalLM(\n",
            "  (model): LlamaModel(\n",
            "    (embed_tokens): Embedding(49152, 576)\n",
            "    (layers): ModuleList(\n",
            "      (0-29): 30 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaAttention(\n",
            "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
            "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=576, out_features=1229, bias=False)\n",
            "          (up_proj): Linear(in_features=576, out_features=1229, bias=False)\n",
            "          (down_proj): Linear(in_features=1229, out_features=576, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
            "    (rotary_emb): LlamaRotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6qEmvooZycx"
      },
      "source": [
        "#Upload the model to HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2Ll_kqe5QzO",
        "outputId": "f20dc674-5c74-40d2-ec9d-184701643707"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pruned model saved to ./pruned20-llama-1b-st\n"
          ]
        }
      ],
      "source": [
        "new_model_name = 'pruned20-llama-1b-st'\n",
        "output_dir = './'+new_model_name\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Pruned model saved to {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3LjjsGZV5ZHJ"
      },
      "outputs": [
        {
          "ename": "HfHubHTTPError",
          "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-68b17b9d-7f09dd0300c55d5e29c6f721;ee9fccae-447f-4e7b-9056-edbfbbc16bfe)\n\nInvalid username or password.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Nextcloud/Knowledge Base/Projects/OMEGAPROJ/optim/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Nextcloud/Knowledge Base/Projects/OMEGAPROJ/optim/.venv/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Push the model to your Hugging Face repository\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Nextcloud/Knowledge Base/Projects/OMEGAPROJ/optim/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4230\u001b[39m, in \u001b[36mPreTrainedModel.push_to_hub\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   4228\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tags:\n\u001b[32m   4229\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] = tags\n\u001b[32m-> \u001b[39m\u001b[32m4230\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Nextcloud/Knowledge Base/Projects/OMEGAPROJ/optim/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:956\u001b[39m, in \u001b[36mPushToHubMixin.push_to_hub\u001b[39m\u001b[34m(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)\u001b[39m\n\u001b[32m    953\u001b[39m repo_url = deprecated_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mrepo_url\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    954\u001b[39m organization = deprecated_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33morganization\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m repo_id = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_repo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m=\u001b[49m\u001b[43morganization\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;66;03m# Create a new empty model card and eventually tag it\u001b[39;00m\n\u001b[32m    961\u001b[39m model_card = create_and_tag_model_card(\n\u001b[32m    962\u001b[39m     repo_id, tags, token=token, ignore_metadata_errors=ignore_metadata_errors\n\u001b[32m    963\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Nextcloud/Knowledge Base/Projects/OMEGAPROJ/optim/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:760\u001b[39m, in \u001b[36mPushToHubMixin._create_repo\u001b[39m\u001b[34m(self, repo_id, private, token, repo_url, organization)\u001b[39m\n\u001b[32m    757\u001b[39m             repo_id = repo_id.split(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m]\n\u001b[32m    758\u001b[39m         repo_id = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morganization\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m url = \u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m url.repo_id\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Nextcloud/Knowledge Base/Projects/OMEGAPROJ/optim/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Nextcloud/Knowledge Base/Projects/OMEGAPROJ/optim/.venv/lib/python3.12/site-packages/huggingface_hub/hf_api.py:3755\u001b[39m, in \u001b[36mHfApi.create_repo\u001b[39m\u001b[34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[39m\n\u001b[32m   3752\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   3754\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3755\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3756\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   3757\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err.response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m   3758\u001b[39m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Nextcloud/Knowledge Base/Projects/OMEGAPROJ/optim/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[31mHfHubHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-68b17b9d-7f09dd0300c55d5e29c6f721;ee9fccae-447f-4e7b-9056-edbfbbc16bfe)\n\nInvalid username or password."
          ]
        }
      ],
      "source": [
        "# Push the model to your Hugging Face repository\n",
        "\n",
        "model.push_to_hub(new_model_name, private=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xNN-aYa5h9B"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(new_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdKFR5Ju23kI"
      },
      "source": [
        "#Evaluating models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UM2pkqFAYEe"
      },
      "source": [
        "In this section, we'll take a look at some standard evaluations in the world of Large Language Models using the lm-evaluation library from EleutherAI.\n",
        "\n",
        "Specifically, we'll use LAMBADA and BoolQ. Since the pruning performed could be considered structuralâ€”that is, it affects the model's overall structure without a specific targetâ€”Iâ€™ve chosen two rather different evaluation tasks.\n",
        "\n",
        "I want to remind you that the goal of this notebook is to demonstrate the pruning process, so I wonâ€™t be doing a comprehensive study of how it impacts performance; that will be saved for a future article. Additionally, these models are designed to be fine-tuned before being used.\n",
        "\n",
        "However, I believe that seeing how pruning impacts model performance can help illustrate the pruning process itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPOg8Hoa22xA"
      },
      "outputs": [],
      "source": [
        "!pip install -q lm-eval\n",
        "from lm_eval import evaluator, tasks, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5wp8lM63IGz"
      },
      "outputs": [],
      "source": [
        "def evaluate_hf_model(model_name, tasks=['arc_easy'], num_fewshot=0):\n",
        "    \"\"\"\n",
        "    It calls the evaluator to evaluate a model available on Hugging Face.\n",
        "\n",
        "    Args:\n",
        "    - model_name: The model name in hugging Face.\n",
        "    - tasks: Tasks to evaluate.\n",
        "    - num_fewshot: Number of examples of few-shot learning\n",
        "\n",
        "    Returns:\n",
        "    - metrics.\n",
        "    \"\"\"\n",
        "    model_args = f\"pretrained={model_name},device=cuda\"\n",
        "    tasks = tasks\n",
        "\n",
        "    results = evaluator.simple_evaluate(\n",
        "      model=\"hf\",\n",
        "      model_args=model_args,\n",
        "      tasks=tasks,\n",
        "      num_fewshot=0,  # Number of few-shot smaples.\n",
        "      limit=None,  # Use all the samples in the Evaluate Dataset.\n",
        "      bootstrap_iters=10\n",
        "    )\n",
        "\n",
        "    metrics = results.get('results', {})\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZm8VvA33Nh6"
      },
      "outputs": [],
      "source": [
        "# Select tasks to evaluate.\n",
        "tasks = ['lambada', 'boolq']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTW3E3mp145e"
      },
      "outputs": [],
      "source": [
        "metrics_base = evaluate_hf_model(\"meta-llama/Llama-3.2-1B\", tasks=tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-g3vyPN3VZp"
      },
      "outputs": [],
      "source": [
        "metrics_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aN5KeIkQ15gM"
      },
      "outputs": [],
      "source": [
        "metrics_pruned = evaluate_hf_model(\"oopere/pruned40-llama-1b\", tasks=tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bboB7uU39l_Z"
      },
      "outputs": [],
      "source": [
        "metrics_pruned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWNOHoVdpgcP"
      },
      "source": [
        "![My Image](https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/img/lambada_BooQ_Accuracy.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c1FodzbrwMv"
      },
      "source": [
        "As we can see, the effect of pruning has been somewhat asymmetrical. The tasks evaluated by the BoolQ test havenâ€™t experienced significant degradationâ€”only about a 2% drop for a model that lost 35% of its weight.\n",
        "\n",
        "In contrast, the impact on the Lambada test has been remarkable, with a drop in accuracy of over 50%.\n",
        "\n",
        "This indicates that the model retains much of its comprehension ability but struggles with tests requiring more open-ended generation.\n",
        "\n",
        "BoolQ simply presents the model with a text and a question to be answered with Yes/No. Itâ€™s a test focused on measuring the modelâ€™s ability to understand relationships within the input text.\n",
        "\n",
        "Lambada, on the other hand, asks the model to guess the last word of a paragraph, a complex task where the final word tests the modelâ€™s capability in complex language modeling.\n",
        "\n",
        "These results are consistent with the functionality of the MLP layers that were pruned.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MwoMVHLZ7ad"
      },
      "source": [
        "#Conclusion.\n",
        "This time, we successfully pruned the Llama model correctly. This same procedure could be applied to any model that shares this structure, regardless of its size.\n",
        "\n",
        "Weâ€™ve managed to reduce the modelâ€™s size while, at least initially, preserving much of its functionality, depending on the % pruned and the task demanded to the model.\n",
        "\n",
        "Itâ€™s important to remember that a pruned model doesnâ€™t typically have direct application on its own; rather, it often serves as the foundation for a new model obtained through further training.\n",
        "\n",
        "## Future Work.\n",
        "The first three notebooks of the course have focused on a type of structured pruning that removes neurons deemed less important.\n",
        "\n",
        "We should explore other forms of structured pruning, such as removing entire layers, as well as different ways to determine which elements are pruned from the model. One such method is Activation-Based Pruning, where neuron activations are evaluated using a specific dataset, and those with low activation are removed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW-biFT6U0zQ"
      },
      "source": [
        "##Authors Note.\n",
        "In addition to creating content like this notebook and offering it under the MIT license, I have also contributed to repositories such as those of Hugging Face and Google Gemini.\n",
        "\n",
        "I am especially proud of my book: <a href=\"https://amzn.to/4eanT1g\"><b>Large Language Models:</b> Apply and Implement Strategies for Large Language Models</a> (Apress).\n",
        "\n",
        "You can find it on both <a href=\"https://amzn.to/4eanT1g\">Amazon</a> and <a href=\"https://link.springer.com/book/10.1007/979-8-8688-0515-8\">Springer</a>, where they often have good deals on the purchase price.\n",
        "\n",
        "If you take a look and end up purchasing it, keep in mind that you can reach out with any questions via the Discussions section of this same repository or on any of my social media channels. Iâ€™ll do my best to respond as quickly as possible."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "optim",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
