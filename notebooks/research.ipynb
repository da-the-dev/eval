{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5071d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e443b87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "device = \"mps\"\n",
    "\n",
    "model: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2502852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(prompt, model=model, tokenizer=tokenizer):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        temperature=None,\n",
    "        top_p=None,\n",
    "        do_sample=False,          # Disable sampling\n",
    "        num_beams=5,              # Use beam search\n",
    "        early_stopping=True,      # Stop when end-of-sequence token is generated\n",
    "        no_repeat_ngram_size=2    # Prevent repetition of 2-grams\n",
    "    )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc112a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Paris is the capital of France and the largest city in the country. It is located on the banks of the Seine River. The city has a population of about 1.5 million people.\n",
      "\n",
      "The history of Paris dates back to the\n"
     ]
    }
   ],
   "source": [
    "# Test the original model\n",
    "prompt = \"Paris is the capital of\"\n",
    "generated = get_output(prompt)\n",
    "print(f\"Generated text: {generated}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3ee57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea7e134b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model parameters: 134515008\n"
     ]
    }
   ],
   "source": [
    "original_param_count = count_params(model)\n",
    "print(f\"Original model parameters: {original_param_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3aadc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neuron_pair_importance(gate_weight: torch.Tensor, up_weight: torch.Tensor):\n",
    "    gate_max_abs = torch.max(gate_weight, dim=1).values + torch.abs(torch.min(gate_weight, dim=1).values)\n",
    "    up_max_abs = torch.max(up_weight, dim=1).values + torch.abs(torch.min(up_weight, dim=1).values)\n",
    "    importance_scores = gate_max_abs + up_max_abs\n",
    "    return importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5bf8bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_grained_prune(tensor: torch.Tensor, sparsity : float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    magnitude-based pruning for single tensor\n",
    "    :param tensor: torch.(cuda.)Tensor, weight of conv/fc layer\n",
    "    :param sparsity: float, pruning sparsity\n",
    "        sparsity = #zeros / #elements = 1 - #nonzeros / #elements\n",
    "    :return:\n",
    "        torch.(cuda.)Tensor, mask for zeros\n",
    "    \"\"\"\n",
    "    sparsity = min(max(0.0, sparsity), 1.0)\n",
    "    if sparsity == 1.0:\n",
    "        tensor.zero_()\n",
    "        return torch.zeros_like(tensor)\n",
    "    elif sparsity == 0.0:\n",
    "        return torch.ones_like(tensor)\n",
    "\n",
    "    num_elements = tensor.numel()\n",
    "\n",
    "    ##################### YOUR CODE STARTS HERE #####################\n",
    "    # Step 1: calculate the #zeros (please use round())\n",
    "    num_zeros = round(sparsity * num_elements)\n",
    "    \n",
    "    if num_zeros == 0:\n",
    "        return torch.ones_like(tensor)\n",
    "    \n",
    "    # Step 2: calculate the importance of weight\n",
    "    importance = tensor.abs()\n",
    "    # Step 3: calculate the pruning threshold\n",
    "    threshold = torch.kthvalue(importance.view(-1), num_zeros).values\n",
    "    \n",
    "    # Step 4: get binary mask (1 for nonzeros, 0 for zeros)\n",
    "    mask = (importance > threshold).to(tensor.dtype)\n",
    "    ##################### YOUR CODE ENDS HERE #######################\n",
    "\n",
    "    # Step 5: apply mask to prune the tensor\n",
    "    tensor.mul_(mask)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eebdb87",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LlamaMLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprune_neuron_pairs\u001b[39m(mlp: \u001b[43mLlamaMLP\u001b[49m, prune_percent: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m      2\u001b[39m     gate_weight = mlp.gate_proj.weight.data.float()\n\u001b[32m      3\u001b[39m     up_weight = mlp.up_proj.weight.data.float()\n",
      "\u001b[31mNameError\u001b[39m: name 'LlamaMLP' is not defined"
     ]
    }
   ],
   "source": [
    "def prune_neuron_pairs(mlp: LlamaMLP, prune_percent: float):\n",
    "    gate_weight = mlp.gate_proj.weight.data.float()\n",
    "    up_weight = mlp.up_proj.weight.data.float()\n",
    "\n",
    "\n",
    "    importance_scores = compute_neuron_pair_importance(gate_weight, up_weight)\n",
    "\n",
    "\n",
    "    original_intermediate_size = gate_weight.size(0)\n",
    "    #Computes the number of neurons to prune.\n",
    "    num_neuron_pairs_to_prune = min(int(prune_percent * original_intermediate_size), original_intermediate_size - 1)\n",
    "    #Calculate the number of neurons to keep. The new intermediate size.\n",
    "    k = original_intermediate_size - num_neuron_pairs_to_prune\n",
    "\n",
    "    #Just check that there is no big error calculating k. We can't prune all the neurons.\n",
    "    if k <= 0:\n",
    "        raise ValueError(f\"Invalid number of neuron pairs to keep: {k}. Adjust the prune_percent.\")\n",
    "\n",
    "    #Select the neuros to keep, by obtaining the indices to keep.\n",
    "    _, indices_to_keep = torch.topk(importance_scores, k, largest=True, sorted=True)\n",
    "    indices_to_keep = indices_to_keep.sort().values\n",
    "\n",
    "    #create the new layers\n",
    "    new_gate_proj = nn.Linear(mlp.gate_proj.in_features, k, bias=False).to(device)\n",
    "    new_up_proj = nn.Linear(mlp.up_proj.in_features, k, bias=False).to(device)\n",
    "    new_down_proj = nn.Linear(k, mlp.down_proj.out_features, bias=False).to(device)\n",
    "\n",
    "    #copy weights to the new layers.\n",
    "    new_gate_proj.weight.data = mlp.gate_proj.weight.data[indices_to_keep, :]\n",
    "    new_up_proj.weight.data = mlp.up_proj.weight.data[indices_to_keep, :]\n",
    "    new_down_proj.weight.data = mlp.down_proj.weight.data[:, indices_to_keep]\n",
    "\n",
    "    #return new layers and intermediate size.\n",
    "    return new_gate_proj, new_up_proj, new_down_proj, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6148f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(model: LlamaForCausalLM, prune_percent: float):\n",
    "    new_intermediate_size = None\n",
    "\n",
    "    for idx, layer in enumerate(model.model.layers):\n",
    "        mlp: LlamaMLP = layer.mlp\n",
    "\n",
    "        new_gate_proj, new_up_proj, new_down_proj, new_size = prune_neuron_pairs(mlp, prune_percent)\n",
    "\n",
    "        mlp.gate_proj = new_gate_proj\n",
    "        mlp.up_proj = new_up_proj\n",
    "        mlp.down_proj = new_down_proj\n",
    "\n",
    "        if new_intermediate_size is None:\n",
    "            new_intermediate_size = new_size\n",
    "\n",
    "    model.config.intermediate_size = new_intermediate_size\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_percent = 0.05  # Prune 20% of neurons\n",
    "model = update_model(model, prune_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1a4dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned model parameters: 126583488\n",
      "Reduction in parameters: 7931520\n",
      "Percentage of weight savings: 5.90%\n"
     ]
    }
   ],
   "source": [
    "# Recalculate the number of parameters\n",
    "pruned_param_count = count_params(model)\n",
    "reduction_in_params = original_param_count - pruned_param_count\n",
    "percentage_savings = (reduction_in_params / original_param_count) * 100\n",
    "\n",
    "print(f\"Pruned model parameters: {pruned_param_count}\")\n",
    "print(f\"Reduction in parameters: {reduction_in_params}\")\n",
    "print(f\"Percentage of weight savings: {percentage_savings:.2f}%\")\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5bcc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text after pruning: Paris is the capital of of,,.\"\n",
      "\n",
      "_\n",
      "\n",
      "\n",
      "\n",
      "-__-\n",
      " -_ --- -1-3-2-4-5-6-7-8-9-101123\n"
     ]
    }
   ],
   "source": [
    "# Test the pruned model\n",
    "generated = get_output(prompt, model, tokenizer)\n",
    "print(f\"Generated text after pruning: {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241dde71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(49152, 576)\n",
      "    (layers): ModuleList(\n",
      "      (0-29): 30 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
      "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=576, out_features=1229, bias=False)\n",
      "          (up_proj): Linear(in_features=576, out_features=1229, bias=False)\n",
      "          (down_proj): Linear(in_features=1229, out_features=576, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df6166",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
