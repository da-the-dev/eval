{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d28423",
   "metadata": {},
   "source": [
    "## Evaluation framework\n",
    "The goal of this notebook is to create an evaluation toolkit to measure LLM performance.\n",
    "\n",
    "Key notable metrics:\n",
    "- Language metrics\n",
    "  - Perplexity\n",
    "  - BLUE\n",
    "  - ROUGE\n",
    "  - Other metrics (TODO Define)\n",
    "- Task-specific metrics\n",
    "  - Different benchmarks (TODO Define what benchmarks)\n",
    "- Inference metrics\n",
    "  - Time To First Token (TTFT)\n",
    "  - Memory footprint\n",
    "\n",
    "This framework is to be used in a future work implementing prunning-training approach to model compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d92912",
   "metadata": {},
   "source": [
    "## Abstract usage flow\n",
    "- A dataset is selected\n",
    "- A model is selected and loaded\n",
    "- Hyperparameters for a given model are fixed\n",
    "- Dataset is parsed through the model and the metrics are calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aea2da4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d190f0f",
   "metadata": {},
   "source": [
    "### A dirty first example\n",
    "Let's consider 2 LLMs:\n",
    "- https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct\n",
    "- https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "\n",
    "Note: these are **\"instruct\"** models\n",
    "\n",
    "Let's select the metrics to evaluate these models:\n",
    "- Language metrics\n",
    "  - Perplexity\n",
    "  - BLUE\n",
    "  - ROUGE\n",
    "- Task-specific metrics\n",
    "  - MMLU\n",
    "  - Winograde\n",
    "\n",
    "Each model has values for each metrics published on their model card. Let's try to replicate the numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3b14a7",
   "metadata": {},
   "source": [
    "#### Language metrics\n",
    "\n",
    "As noted above, let's first compute Perplexity, BLUE score, and ROUGE for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1610ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba5ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'HuggingFaceTB/SmolLM2-135M-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f36c87",
   "metadata": {},
   "source": [
    "> Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. If we have a tokenized sequence:     \n",
    "> $$X=(x_0, x_1, \\dots, x_t)$$\n",
    "> then the perplexity of $X$ is\n",
    "> $$\n",
    "\\exp \\left\\{ -\\dfrac{1}{t}\\sum^{t} _{i}\\log p_{\\theta}(x_{i}|x_{<i})\\right\\}  \n",
    "> $$\n",
    "> where $log p_{\\theta}(x_{i}|x_{<i})$ is the log-likelihood of the $i$th token conditioned on the preceding tokens $x_{<i}$ \n",
    ">\n",
    "> Intuitively, it can be thought of as an evaluation of the model’s ability to predict uniformly among the set of specified tokens in a corpus. Importantly, this means that the tokenization procedure has a direct impact on a model’s perplexity which should always be taken into consideration when comparing different models.\n",
    "\n",
    "[Reference](https://huggingface.co/docs/transformers/perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674dd20b",
   "metadata": {},
   "source": [
    "For evaluation we will be using the Wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99e3bed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (304978 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddeeabac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def ppl(model, encodings, max_length = 1024, stride = 512):\n",
    "    seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "    nll_sum = 0.0\n",
    "    n_tokens = 0\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "        trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc].to('mps')\n",
    "        target_ids = input_ids.clone()\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "            # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "            # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "            # to the left by 1.\n",
    "            neg_log_likelihood = outputs.loss\n",
    "\n",
    "        # Accumulate the total negative log-likelihood and the total number of tokens\n",
    "        num_valid_tokens = (target_ids != -100).sum().item()  # number of valid tokens in target_ids\n",
    "        batch_size = target_ids.size(0)\n",
    "        num_loss_tokens = num_valid_tokens - batch_size  # subtract batch_size due to internal label shift\n",
    "        nll_sum += neg_log_likelihood * num_loss_tokens\n",
    "        n_tokens += num_loss_tokens\n",
    "\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "\n",
    "    avg_nll = nll_sum / n_tokens  # average negative log-likelihood per token\n",
    "    return torch.exp(avg_nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52654eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 594/596 [01:06<00:00,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.6064, device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "smol_135_ppl = ppl(model, encodings)\n",
    "print(smol_135_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f84d5164",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name2 = 'meta-llama/Llama-3.1-8B-Instruct'\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(model_name)\n",
    "model2 = AutoModelForCausalLM.from_pretrained(model_name).to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "223a8783",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 594/596 [01:06<00:00,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.6064, device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "llama31_ppl = ppl(model2, encodings)\n",
    "print(llama31_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780424cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
